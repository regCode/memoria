{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preentrenamiento - Wang - bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def clean_str(self, string):\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for all datasets except for SST.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "        return string.strip().lower()\n",
    "    \n",
    "    #################################################################\n",
    "    ##################### 20 Newsgroups #############################\n",
    "    #################################################################\n",
    "    \n",
    "    path_train_20newsgroups = \"../20NewsGroup/20ng-train-stemmed.txt\"\n",
    "    path_test_20newsgroups = \"../20NewsGroup/20ng-test-stemmed.txt\"\n",
    "    \n",
    "    target_names_20newsgroups = [\n",
    "        \"alt.atheism\", \n",
    "        \"comp.graphics\",\n",
    "        \"comp.os.ms-windows.misc\",\n",
    "        \"comp.sys.ibm.pc.hardware\", \n",
    "        \"comp.sys.mac.hardware\",\n",
    "        \"comp.windows.x\",\n",
    "        \"misc.forsale\",\n",
    "        \"rec.autos\",\n",
    "        \"rec.motorcycles\",\n",
    "        \"rec.sport.baseball\",\n",
    "        \"rec.sport.hockey\",\n",
    "        \"sci.crypt\",\n",
    "        \"sci.electronics\",\n",
    "        \"sci.med\",\n",
    "        \"sci.space\",\n",
    "        \"soc.religion.christian\",\n",
    "        \"talk.politics.guns\",\n",
    "        \"talk.politics.mideast\",\n",
    "        \"talk.politics.misc\",\n",
    "        \"talk.religion.misc\"\n",
    "    ]\n",
    "    \n",
    "    def read_20newsgroups_file(self, path_test_20newsgroups):\n",
    "        data = []\n",
    "        target = []\n",
    "        \n",
    "        with open(path_test_20newsgroups) as file:\n",
    "            for index, line in enumerate(file):\n",
    "                tokens_count = len(line.split())\n",
    "\n",
    "                if tokens_count > 1 and tokens_count <= 301:\n",
    "                    category, text = line.split(None, 1)\n",
    "                    data.append(self.clean_str(text))\n",
    "                    target.append(self.target_names_20newsgroups.index(category))\n",
    "                \n",
    "        return data, target\n",
    "        \n",
    "    \n",
    "    def fetch_20newsgroups(self, subset = \"train\"):\n",
    "        dataset = {'data': None,  'target': None , 'target_names': self.target_names_20newsgroups}\n",
    "    \n",
    "        if subset == 'train':\n",
    "            dataset['data'], dataset['target'] = self.read_20newsgroups_file(self.path_train_20newsgroups)\n",
    "        elif subset == 'test':\n",
    "            dataset['data'], dataset['target'] = self.read_20newsgroups_file(self.path_test_20newsgroups)\n",
    "        elif subset == 'all':\n",
    "            data_train, target_train = self.read_20newsgroups_file(self.path_train_20newsgroups)\n",
    "            data_test, target_test = self.read_20newsgroups_file(self.path_test_20newsgroups)\n",
    "            \n",
    "            dataset['data'], dataset['target'] = data_train + data_test, target_train + target_test\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    #################################################################\n",
    "    ##################### SearchSnippets ############################\n",
    "    #################################################################\n",
    "    \n",
    "    path_train_search_snippets = \"../SearchSnippets/train.txt\"\n",
    "    path_test_search_snippets = \"../SearchSnippets/test.txt\"\n",
    "    \n",
    "    target_names_search_snippets = [\n",
    "        \"business\",\n",
    "        \"computers\",\n",
    "        \"culture-arts-entertainment\",\n",
    "        \"education-science\",\n",
    "        \"engineering\",\n",
    "        \"health\",\n",
    "        \"politics-society\",\n",
    "        \"sports\"\n",
    "    ]\n",
    "    \n",
    "    def read_search_snippets_file(self, path_test_search_snippets):\n",
    "        data = []\n",
    "        target = []\n",
    "        \n",
    "        with open(path_test_search_snippets, encoding=\"utf8\") as file:\n",
    "            for index, line in enumerate(file):\n",
    "                tokens_count = len(line.split())\n",
    "               \n",
    "                if tokens_count > 1 and tokens_count <= 301:\n",
    "                    text, category = line.rsplit(None, 1)\n",
    "                    data.append(self.clean_str(text))\n",
    "                    target.append(self.target_names_search_snippets.index(category))\n",
    "                \n",
    "        return data, target\n",
    "        \n",
    "    \n",
    "    def fetch_search_snippets(self, subset = \"train\"):\n",
    "        dataset = {'data': None,  'target': None , 'target_names': self.target_names_search_snippets}\n",
    "    \n",
    "        if subset == 'train':\n",
    "            dataset['data'], dataset['target'] = self.read_search_snippets_file(self.path_train_search_snippets)\n",
    "        elif subset == 'test':\n",
    "            dataset['data'], dataset['target'] = self.read_search_snippets_file(self.path_test_search_snippets)\n",
    "        elif subset == 'all':\n",
    "            data_train, target_train = self.read_search_snippets_file(self.path_train_search_snippets)\n",
    "            data_test, target_test = self.read_search_snippets_file(self.path_test_search_snippets)\n",
    "            \n",
    "            dataset['data'], dataset['target'] = data_train + data_test, target_train + target_test\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos de Preentrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import threading\n",
    "import math as mt\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "class binaryCodes:\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    def __init__(self, dataset, sigma = 2, method = 'weiss', k_neighbors = 7, c_coeff = 1, \n",
    "                 a_equal_coeff = 1, b_unequal_coeff = 0.1):\n",
    "        self.tf_idf_features_matrix = self.tf_idf(dataset['data'])\n",
    "        self.distance_matrix = self.euclidean_distance(self.tf_idf_features_matrix)\n",
    "        self.labels = dataset['target']\n",
    "        self.sigma = sigma\n",
    "        self.method = method\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.c_coeff = c_coeff\n",
    "        self.a_equal_coeff = a_equal_coeff\n",
    "        self.b_unequal_coeff = b_unequal_coeff\n",
    "        \n",
    "    def tf_idf(self, data):\n",
    "        return self.vectorizer.fit_transform(data)\n",
    "    \n",
    "    def euclidean_distance(self, tf_idf_features):\n",
    "        return euclidean_distances(tf_idf_features, tf_idf_features)\n",
    "    \n",
    "    def parallel_operation(self, row_interval, w):\n",
    "        for row in range(row_interval[0], row_interval[1]):\n",
    "            for column in range(row, w.shape[1]):\n",
    "                \n",
    "                if self.method == 'weiss':\n",
    "                    value = np.exp(-self.distance_matrix[row][column]**2/self.sigma**2)\n",
    "                    \n",
    "                elif self.method == 'chinese_weiss':\n",
    "                  \n",
    "                    row_neighbors = self.distance_matrix[row].argsort()[1:self.k_neighbors+1]\n",
    "                    column_neighbors = self.distance_matrix[column].argsort()[1:self.k_neighbors+1]\n",
    "                    \n",
    "                    if row in column_neighbors or column in row_neighbors:\n",
    "                        value = self.c_coeff*np.exp(-self.distance_matrix[row][column]**2/(2*self.sigma**2))\n",
    "                    else:\n",
    "                        value = 0\n",
    "                        \n",
    "                elif self.method == 'chinese_weiss_supervised':\n",
    "                    \n",
    "                    row_neighbors = self.distance_matrix[row].argsort()[1:self.k_neighbors+1]\n",
    "                    column_neighbors = self.distance_matrix[column].argsort()[1:self.k_neighbors+1]\n",
    "                    \n",
    "                    if row in column_neighbors or column in row_neighbors:\n",
    "                        if self.labels[row] == self.labels[column]:\n",
    "                            value = self.a_equal_coeff*np.exp(-self.distance_matrix[row][column]**2/(2*self.sigma**2))\n",
    "                        else:\n",
    "                            value = self.b_unequal_coeff*np.exp(-self.distance_matrix[row][column]**2/(2*self.sigma**2))\n",
    "                    else:\n",
    "                        value = 0\n",
    "                    \n",
    "                w[row, column] = value\n",
    "                \n",
    "                if row != column:\n",
    "                    w[column, row] = value\n",
    "        \n",
    "    def parallel_operation2(self, row_interval, w_matrix, d):\n",
    "        for row in range(row_interval[0], row_interval[1]):\n",
    "            sum_row = np.sum(w_matrix[row])\n",
    "            d[row, row] = sum_row\n",
    "                \n",
    "    def w_matrix(self):\n",
    "        \n",
    "        print(\"Calculating W matrix using {} method ...\".format(self.method))\n",
    "        \n",
    "        data_size, _ = self.tf_idf_features_matrix.shape\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        w = np.zeros((data_size, data_size))\n",
    "\n",
    "        cores_count = multiprocessing.cpu_count()\n",
    "        bounds = list(range(mt.floor(data_size/cores_count), \n",
    "                            data_size-data_size%cores_count+1, mt.floor(data_size/cores_count)))\n",
    "\n",
    "        bounds[-1] += data_size%cores_count\n",
    "\n",
    "        threads = list()\n",
    "\n",
    "        lower_bound = 0\n",
    "\n",
    "        for index, upper_bound in enumerate(bounds):\n",
    "            threads.append(threading.Thread(target = self.parallel_operation, args=([lower_bound, upper_bound], w)))\n",
    "            lower_bound = upper_bound\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(\"Time to compute: {} min\".format(elapsed_time/60))\n",
    "\n",
    "        print(\"Nonzero elements count: {}\".format(np.count_nonzero(w)))\n",
    "        print(\"Ratio: {}\".format(np.count_nonzero(w)/data_size**2))\n",
    "\n",
    "        return w\n",
    "    \n",
    "    def d_matrix(self, w_matrix):\n",
    "        \n",
    "        print(\"Calculating D matrix using {} method ...\".format(self.method))\n",
    "        \n",
    "        data_size, _ = self.tf_idf_features_matrix.shape\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        d = np.zeros((data_size, data_size))\n",
    "\n",
    "        cores_count = multiprocessing.cpu_count()\n",
    "        bounds = list(range(mt.floor(data_size/cores_count), \n",
    "                            data_size-data_size%cores_count+1, mt.floor(data_size/cores_count)))\n",
    "\n",
    "        bounds[-1] += data_size%cores_count\n",
    "\n",
    "        threads = list()\n",
    "\n",
    "        lower_bound = 0\n",
    "\n",
    "        for index, upper_bound in enumerate(bounds):\n",
    "            threads.append(threading.Thread(target = self.parallel_operation2, \n",
    "                                            args=([lower_bound, upper_bound], w_matrix, d)))\n",
    "            lower_bound = upper_bound\n",
    "            \n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(\"Time to compute: {} min\".format(elapsed_time/60))\n",
    "\n",
    "        return d\n",
    "    \n",
    "    def binary_codes(self, hash_len = 64):\n",
    "        \n",
    "        data_size, _ = self.tf_idf_features_matrix.shape\n",
    "        sigma_str = str(self.sigma).replace('.', '')\n",
    "        c_coeff_str = str(self.c_coeff).replace('.', '')\n",
    "        a_equal_coeff_str = str(self.a_equal_coeff).replace('.', '')\n",
    "        b_unequal_coeff_str = str(self.b_unequal_coeff).replace('.', '')\n",
    "        \n",
    "        try:\n",
    "            hash_codes = np.load('hash_codes-{}_{}_{}_{}_{}_{}_{}_{}.npy'.format(data_size, hash_len, self.method, \n",
    "                                     sigma_str, c_coeff_str, a_equal_coeff_str, b_unequal_coeff_str, self.k_neighbors))    \n",
    "            \n",
    "            print(\"Using preexisting hash codes\")\n",
    "\n",
    "        except:\n",
    "        \n",
    "            print(\"Calculating hash codes\")\n",
    "            global_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                eigen_values = np.load('eigen_values-{}_{}_{}_{}_{}_{}_{}.npy'.format(data_size, self.method, sigma_str, \n",
    "                                                        c_coeff_str, a_equal_coeff_str, b_unequal_coeff_str, self.k_neighbors)) \n",
    "                eigen_vectors = np.load('eigen_vectors-{}_{}_{}_{}_{}_{}_{}.npy'.format(data_size, self.method, sigma_str, \n",
    "                                                        c_coeff_str, a_equal_coeff_str, b_unequal_coeff_str, self.k_neighbors))\n",
    "                \n",
    "                print(\"Using preexisting eigen vectors and values\")\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                w_matrix = self.w_matrix()\n",
    "                d_matrix = self.d_matrix(w_matrix)\n",
    "\n",
    "                sub_martix = d_matrix-w_matrix\n",
    "                \n",
    "                print(\"Calculating eigen vectors\")\n",
    "                \n",
    "                eigen_values, eigen_vectors = np.linalg.eig(sub_martix)\n",
    "                \n",
    "                np.save('eigen_values-{}_{}_{}_{}_{}_{}_{}.npy'.format(data_size, self.method, sigma_str, \n",
    "                                        c_coeff_str, a_equal_coeff_str, b_unequal_coeff_str, self.k_neighbors), eigen_values)\n",
    "                \n",
    "                np.save('eigen_vectors-{}_{}_{}_{}_{}_{}_{}.npy'.format(data_size, self.method, sigma_str, \n",
    "                                        c_coeff_str, a_equal_coeff_str, b_unequal_coeff_str, self.k_neighbors), eigen_vectors)\n",
    "\n",
    "            eigen_values = np.delete(eigen_values, 0)\n",
    "            eigen_vectors = np.delete(eigen_vectors, 0, 1)\n",
    "\n",
    "            min_eigen_values = np.argsort(eigen_values)\n",
    "           \n",
    "            count = 0\n",
    "            hash_codes = list()\n",
    "\n",
    "            for i in range(hash_len):\n",
    "                min_eigen_values[count]\n",
    "                hash_codes.append(eigen_vectors[:, min_eigen_values[count]])\n",
    "                count += 1\n",
    "\n",
    "            hash_codes = np.asarray(hash_codes).transpose()\n",
    "\n",
    "            threshold = lambda x: 1 if x>=0 else 0 \n",
    "            vfunc = np.vectorize(threshold)\n",
    "\n",
    "            hash_codes = vfunc(hash_codes)\n",
    "\n",
    "            elapsed_time = time.time() - global_start_time\n",
    "\n",
    "            print(\"Total Time: {} min \\n\".format(elapsed_time/60))\n",
    "\n",
    "            np.save('hash_codes-{}_{}_{}_{}_{}_{}_{}_{}.npy'.format(data_size, hash_len, self.method, \n",
    "                        sigma_str, c_coeff_str, a_equal_coeff_str, b_unequal_coeff_str, self.k_neighbors), hash_codes)\n",
    "        \n",
    "        return hash_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación mediante clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import ipython_genutils\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class evaluate:\n",
    "    \n",
    "    def intra_cluster_distance(hash_codes, dataset):\n",
    "        \n",
    "        clusters = dict()\n",
    "        \n",
    "        categories = dataset['target']\n",
    "        categories_names = dataset['target_names']\n",
    "        \n",
    "        for index, value in enumerate(categories):\n",
    "            if value not in clusters.keys():\n",
    "                clusters[value] = [hash_codes[index]]\n",
    "            else:\n",
    "                clusters[value].append(hash_codes[index])\n",
    "                \n",
    "        distance_matrix = DistanceMetric.get_metric('hamming')\n",
    "        table = {'name': [], 'size': [], 'elements': [], 'median': [], 'mean': [], 'std': [], 'min': [], 'max': []}\n",
    "        \n",
    "        for label in clusters.keys():\n",
    "            inner_distances = distance_matrix.pairwise(clusters[label])\n",
    "            inner_distances = inner_distances[np.triu_indices(inner_distances.shape[0], 1)]\n",
    "            \n",
    "            table['name'].append(categories_names[label])\n",
    "            table['size'].append(len(clusters[label]))\n",
    "            table['elements'].append(len(inner_distances))\n",
    "            table['median'].append(np.median(inner_distances))\n",
    "            table['mean'].append(np.mean(inner_distances))\n",
    "            table['std'].append(np.std(inner_distances))\n",
    "            table['min'].append(np.amin(inner_distances))\n",
    "            table['max'].append(np.amax(inner_distances))\n",
    "            \n",
    "        df = pd.DataFrame(data=table)\n",
    "        display(df)\n",
    "        display(df.describe()) \n",
    "        \n",
    "        return inner_distances\n",
    "    \n",
    "    def inter_cluster_distance(hash_codes, dataset):\n",
    "        \n",
    "        clusters = dict()\n",
    "        \n",
    "        categories = dataset['target']\n",
    "        categories_names = dataset['target_names']\n",
    "        \n",
    "        for index, value in enumerate(categories):\n",
    "            if value not in clusters.keys():\n",
    "                clusters[value] = [hash_codes[index]]\n",
    "            else:\n",
    "                clusters[value].append(hash_codes[index])\n",
    "                \n",
    "        distance_matrix = DistanceMetric.get_metric('hamming')\n",
    "        table = {'name': [], 'size': [], 'elements': [], 'median': [], 'mean': [], 'std': [], 'min': [], 'max': []}\n",
    "        \n",
    "        pairs = list(itertools.combinations(clusters.keys(), 2))\n",
    "        \n",
    "        median_matrix = np.zeros((len(categories_names), len(categories_names)))\n",
    "        mean_matrix = np.zeros((len(categories_names), len(categories_names)))\n",
    "        std_matrix = np.zeros((len(categories_names), len(categories_names)))\n",
    "        min_distance_matrix = np.zeros((len(categories_names), len(categories_names)))\n",
    "        max_distance_matrix = np.zeros((len(categories_names), len(categories_names)))\n",
    "        \n",
    "        for label_a, label_b in pairs:\n",
    "            inter_distances = distance_matrix.pairwise(clusters[label_a], clusters[label_b])\n",
    "            median = np.median(inter_distances)\n",
    "            mean = inter_distances.mean()\n",
    "            std = inter_distances.std()\n",
    "            min_distance = inter_distances.min()\n",
    "            max_distance = inter_distances.max()\n",
    "            \n",
    "            median_matrix[label_a, label_b] = median\n",
    "            median_matrix[label_b, label_a] = median\n",
    "            \n",
    "            mean_matrix[label_a, label_b] = mean\n",
    "            mean_matrix[label_b, label_a] = mean\n",
    "            \n",
    "            std_matrix[label_a, label_b] = std\n",
    "            std_matrix[label_b, label_a] = std\n",
    "            \n",
    "            min_distance_matrix[label_a, label_b] = min_distance\n",
    "            min_distance_matrix[label_b, label_a] = min_distance\n",
    "            \n",
    "            max_distance_matrix[label_a, label_b] = max_distance\n",
    "            max_distance_matrix[label_b, label_a] = max_distance\n",
    "            \n",
    "        ## Median\n",
    "        df = pd.DataFrame(median_matrix, columns = categories_names, index = categories_names)\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        \n",
    "        plt.title(\"Median\", fontsize=18)\n",
    "        ttl = ax.title\n",
    "        ttl.set_position([0.5, 1.01])\n",
    "       \n",
    "        sns.heatmap(df, annot=True, vmin=0, vmax=1, ax=ax)\n",
    "        \n",
    "        ## Mean\n",
    "        df = pd.DataFrame(mean_matrix, columns = categories_names, index = categories_names)\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        \n",
    "        plt.title(\"Mean\", fontsize=18)\n",
    "        ttl = ax.title\n",
    "        ttl.set_position([0.5, 1.01])\n",
    "       \n",
    "        sns.heatmap(df, annot=True, vmin=0, vmax=1, ax=ax)\n",
    "        \n",
    "        ## Std\n",
    "        df = pd.DataFrame(std_matrix, columns = categories_names, index = categories_names)\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        \n",
    "        plt.title(\"Std\", fontsize=18)\n",
    "        ttl = ax.title\n",
    "        ttl.set_position([0.5, 1.01])\n",
    "       \n",
    "        sns.heatmap(df, annot=True, vmin=0, vmax=0.1, ax=ax)\n",
    "        \n",
    "        ## Min distance\n",
    "        df = pd.DataFrame(min_distance_matrix, columns = categories_names, index = categories_names)\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        \n",
    "        plt.title(\"Min Distance\", fontsize=18)\n",
    "        ttl = ax.title\n",
    "        ttl.set_position([0.5, 1.01])\n",
    "       \n",
    "        sns.heatmap(df, annot=True, vmin=0, vmax=1, ax=ax)\n",
    "        \n",
    "        ## Max distance\n",
    "        df = pd.DataFrame(max_distance_matrix, columns = categories_names, index = categories_names)\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        \n",
    "        plt.title(\"Max Distance\", fontsize=18)\n",
    "        ttl = ax.title\n",
    "        ttl.set_position([0.5, 1.01])\n",
    "       \n",
    "        sns.heatmap(df, annot=True, vmin=0, vmax=1, ax=ax)\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación mediante infomation retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "class hashingDatabase:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.database = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def add_element(self, element, label):\n",
    "        self.database.append(element)\n",
    "        self.labels.append(label)\n",
    "        \n",
    "    def find_neighbours_position(self, element):\n",
    "        distance_metric = DistanceMetric.get_metric('hamming')\n",
    "        distance_vector = distance_metric.pairwise([element], self.database)\n",
    "        return np.argsort(distance_vector)\n",
    "    \n",
    "    def evaluate_P_K(self, element, label, k = None):\n",
    "        neighbours_position = self.find_neighbours_position(element)[0][:k]\n",
    "        matched = 0\n",
    "        #print(neighbours_position)\n",
    "        for neighbour_position in neighbours_position:\n",
    "            #print('neighbor label: {}'.format(self.labels[neighbour_position]))\n",
    "            if self.labels[neighbour_position] == label:\n",
    "                matched += 1\n",
    "        #print('precision {}'.format(matched/k))        \n",
    "        return matched/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "dict_keys(['data', 'target', 'target_names'])\n",
      "\n",
      "\n",
      "Dataset example:\n",
      "univers violat separ church state dmn kepler unh edu king becom philosoph philosoph becom king write recent ra order and resist care appar post religi flyer entitl soul scroll thought religion spiritu and matter soul insid bathroom stall door school univers hampshir sort newslett assembl hall director campu pose question spiritu each issu and solicit respons includ issu pretti vagu assum put christian care not mention jesu bibl heard defend doesn support religion thi state univers and strong support separ church and state enrag can thi sound scream for parodi give copi your friendli neighbourhood subgeniu preacher luck run mental mincer and hand you back outrag offens and gut bustingli funni parodi you can past origin can stool scroll thought religion spiritu and matter colon you can us thi text wipe mathew\n",
      "\n",
      "\n",
      "Train dataset:\n",
      "Set size: 10443\n",
      "\n",
      "\n",
      "Mean Lenght: 94.3\n",
      "Max Lenght: 300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "print(\"Dataset structure:\")\n",
    "train_dataset = dataset.fetch_20newsgroups(subset = 'train')\n",
    "print(train_dataset.keys())\n",
    "print(\"\\n\")\n",
    "print(\"Dataset example:\")\n",
    "print(train_dataset['data'][0])\n",
    "print(\"\\n\")\n",
    "print(\"Train dataset:\")\n",
    "print(\"Set size: {}\".format(len(train_dataset['data'])))\n",
    "print(\"\\n\")\n",
    "\n",
    "complete_dataset = train_dataset['data']\n",
    "\n",
    "mean_length = sum(len(document.split()) for document in complete_dataset)/len(complete_dataset)\n",
    "max_length = max(map(lambda document: len(document.split()), complete_dataset))\n",
    "\n",
    "print('Mean Lenght: {}'.format(round(mean_length, 1)))\n",
    "print('Max Lenght: {}'.format(max_length))\n",
    "print(\"\\n\")\n",
    "\n",
    "#vocabulary = list()\n",
    "#for document in complete_dataset:\n",
    "#    for word in document.split():\n",
    "#        if word not in vocabulary:\n",
    "#            vocabulary.append(word)\n",
    "#\n",
    "#print('Vocabulary Size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametro $n$ bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametro n bits 16\n",
      "Calculating hash codes\n",
      "Calculating W matrix using chinese_weiss method ...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "import math \n",
    "\n",
    "k = 100\n",
    "parameters = [16, 32, 64]\n",
    "seeds = [1, 4 ,7]\n",
    "\n",
    "precision_mean = []\n",
    "precision_std = []\n",
    "\n",
    "for parameter in parameters:\n",
    "    print('Parametro n bits {}'.format(parameter))\n",
    "    hash_codes = binaryCodes(train_dataset, sigma = 1, k_neighbors = 7 , c_coeff = 1, method = 'chinese_weiss')\n",
    "    binary_codes = hash_codes.binary_codes(hash_len = parameter)\n",
    "    \n",
    "    precision_val = []\n",
    "    \n",
    "    for count, seed in enumerate(seeds):\n",
    "        print('    Conjunto de validacion {}'.format(count+1))\n",
    "        random.seed(seed)\n",
    "        \n",
    "        validation_sample = random.sample(range(len(train_dataset['data'])), math.ceil(len(train_dataset['data'])*0.2))\n",
    "        validation_split = {'binary_code': [], 'target': []}\n",
    "        \n",
    "        database = hashingDatabase()\n",
    "\n",
    "        for index in range(len(train_dataset['data'])):\n",
    "            if index in validation_sample:\n",
    "                validation_split['binary_code'].append(binary_codes[index])\n",
    "                validation_split['target'].append(train_dataset['target'][index])\n",
    "            else:\n",
    "                database.add_element(binary_codes[index], train_dataset['target'][index])\n",
    "\n",
    "        precisions = []\n",
    "        \n",
    "        for index in range(len(validation_split['binary_code'])):\n",
    "            precision = database.evaluate_P_K(validation_split['binary_code'][index], validation_split['target'][index], k)\n",
    "            precisions.append(precision)    \n",
    "            \n",
    "        print(\"    --->n bits {} precision at {} mean precision {}\".format(parameter, k, np.mean(precisions)))\n",
    "\n",
    "        precision_val.append(np.mean(precisions))\n",
    "    \n",
    "    precision_mean.append(np.mean(precision_val))\n",
    "    precision_std.append(np.std(precision_val))\n",
    "    \n",
    "    print(\"Global Mean Precision {} with std {}\\n\".format(np.mean(precision_val), np.std(precision_val)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Mean: {}\".format(precision_mean))\n",
    "print(\"Std: {}\".format( precision_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abscissa = parameters\n",
    "ordinate = list(map(lambda x: x*100, precision_mean))\n",
    "error = list(map(lambda x: x*100, precision_std))\n",
    " \n",
    "plt.style.use('default') #seaborn-paper\n",
    "#plt.errorbar(abscissa, ordinate, error, capsize=4, capthick=0.8, ecolor='black', elinewidth=0.8, \n",
    "#             marker = 'o', color='g', label=r'parametro $\\epsilon$')\n",
    "plt.plot(abscissa, ordinate, marker = 'o', color='g', label=r'parametro $n$ bits')\n",
    "plt.xscale('log', basex=2)\n",
    "plt.xticks(parameters, parameters)\n",
    "plt.xlabel(r'$n$ bits')\n",
    "plt.ylabel('mP@100(%)')\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('20 Newsgroups (Wang)')\n",
    "#plt.rc('grid', linestyle=\":\", color='grey')\n",
    "#plt.grid(True)\n",
    "\n",
    "#for i in range(len(ordinate)):\n",
    "    # Create a formatted string with three spaces, one newline\n",
    "#    ann = r'{0:.2f}$\\pm${0:.2f}'.format(ordinate[i], error[i])\n",
    "#    plt.annotate(ann, xy=(abscissa[i], ordinate[i]), xycoords='data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SearchSnippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "print(\"Dataset structure:\")\n",
    "train_dataset = dataset.fetch_search_snippets(subset = 'train')\n",
    "print(train_dataset.keys())\n",
    "print(\"\\n\")\n",
    "print(\"Dataset example:\")\n",
    "print(train_dataset['data'][0])\n",
    "print(\"\\n\")\n",
    "print(\"Train dataset:\")\n",
    "print(\"Set size: {}\".format(len(train_dataset['data'])))\n",
    "print(\"\\n\")\n",
    "\n",
    "complete_dataset = train_dataset['data']\n",
    "\n",
    "mean_length = sum(len(document.split()) for document in complete_dataset)/len(complete_dataset)\n",
    "max_length = max(map(lambda document: len(document.split()), complete_dataset))\n",
    "\n",
    "print('Mean Lenght: {}'.format(round(mean_length, 1)))\n",
    "print('Max Lenght: {}'.format(max_length))\n",
    "print(\"\\n\")\n",
    "\n",
    "#vocabulary = list()\n",
    "#for document in complete_dataset:\n",
    "#    for word in document.split():\n",
    "#        if word not in vocabulary:\n",
    "#            vocabulary.append(word)\n",
    "#\n",
    "#print('Vocabulary Size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametro $n$ bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import math \n",
    "\n",
    "k = 100\n",
    "parameters = [16, 32, 64]\n",
    "seeds = [1, 4 ,7]\n",
    "\n",
    "precision_mean = []\n",
    "precision_std = []\n",
    "\n",
    "for parameter in parameters:\n",
    "    print('Parametro n bits {}'.format(parameter))\n",
    "    hash_codes = binaryCodes(train_dataset, sigma=1, k_neighbors = 7 , c_coeff = 1, method = 'chinese_weiss')\n",
    "    binary_codes = hash_codes.binary_codes(hash_len = parameter)\n",
    "    \n",
    "    precision_val = []\n",
    "    \n",
    "    for count, seed in enumerate(seeds):\n",
    "        print('    Conjunto de validacion {}'.format(count+1))\n",
    "        random.seed(seed)\n",
    "        \n",
    "        validation_sample = random.sample(range(len(train_dataset['data'])), math.ceil(len(train_dataset['data'])*0.2))\n",
    "        validation_split = {'binary_code': [], 'target': []}\n",
    "        \n",
    "        database = hashingDatabase()\n",
    "\n",
    "        for index in range(len(train_dataset['data'])):\n",
    "            if index in validation_sample:\n",
    "                validation_split['binary_code'].append(binary_codes[index])\n",
    "                validation_split['target'].append(train_dataset['target'][index])\n",
    "            else:\n",
    "                database.add_element(binary_codes[index], train_dataset['target'][index])\n",
    "\n",
    "        precisions = []\n",
    "        \n",
    "        for index in range(len(validation_split['binary_code'])):\n",
    "            precision = database.evaluate_P_K(validation_split['binary_code'][index], validation_split['target'][index], k)\n",
    "            precisions.append(precision)    \n",
    "            \n",
    "        print(\"    --->n bits {} precision at {} mean precision {}\".format(parameter, k, np.mean(precisions)))\n",
    "\n",
    "        precision_val.append(np.mean(precisions))\n",
    "    \n",
    "    precision_mean.append(np.mean(precision_val))\n",
    "    precision_std.append(np.std(precision_val))\n",
    "    \n",
    "    print(\"Global Mean Precision {} with std {}\\n\".format(np.mean(precision_val), np.std(precision_val)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Mean: {}\".format(precision_mean))\n",
    "print(\"Std: {}\".format( precision_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abscissa = parameters\n",
    "ordinate = list(map(lambda x: x*100, precision_mean))\n",
    "error = list(map(lambda x: x*100, precision_std))\n",
    " \n",
    "plt.style.use('default') #seaborn-paper\n",
    "#plt.errorbar(abscissa, ordinate, error, capsize=4, capthick=0.8, ecolor='black', elinewidth=0.8, \n",
    "#             marker = 'o', color='g', label=r'parametro $\\epsilon$')\n",
    "plt.plot(abscissa, ordinate, marker = 'o', color='g', label=r'parametro $n$ bits')\n",
    "plt.xscale('log', basex=2)\n",
    "plt.xticks(parameters, parameters)\n",
    "plt.xlabel(r'$n$ bits')\n",
    "plt.ylabel('mP@100(%)')\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('20 Newsgroups (Wang)')\n",
    "#plt.rc('grid', linestyle=\":\", color='grey')\n",
    "#plt.grid(True)\n",
    "\n",
    "#for i in range(len(ordinate)):\n",
    "    # Create a formatted string with three spaces, one newline\n",
    "#    ann = r'{0:.2f}$\\pm${0:.2f}'.format(ordinate[i], error[i])\n",
    "#    plt.annotate(ann, xy=(abscissa[i], ordinate[i]), xycoords='data')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
