{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def clean_str(self, string):\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for all datasets except for SST.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "        return string.strip().lower()\n",
    "    \n",
    "    #################################################################\n",
    "    ##################### 20 Newsgroups #############################\n",
    "    #################################################################\n",
    "    \n",
    "    path_train_20newsgroups = \"../20NewsGroup/20ng-train-stemmed.txt\"\n",
    "    path_test_20newsgroups = \"../20NewsGroup/20ng-test-stemmed.txt\"\n",
    "    \n",
    "    target_names_20newsgroups = [\n",
    "        \"alt.atheism\", \n",
    "        \"comp.graphics\",\n",
    "        \"comp.os.ms-windows.misc\",\n",
    "        \"comp.sys.ibm.pc.hardware\", \n",
    "        \"comp.sys.mac.hardware\",\n",
    "        \"comp.windows.x\",\n",
    "        \"misc.forsale\",\n",
    "        \"rec.autos\",\n",
    "        \"rec.motorcycles\",\n",
    "        \"rec.sport.baseball\",\n",
    "        \"rec.sport.hockey\",\n",
    "        \"sci.crypt\",\n",
    "        \"sci.electronics\",\n",
    "        \"sci.med\",\n",
    "        \"sci.space\",\n",
    "        \"soc.religion.christian\",\n",
    "        \"talk.politics.guns\",\n",
    "        \"talk.politics.mideast\",\n",
    "        \"talk.politics.misc\",\n",
    "        \"talk.religion.misc\"\n",
    "    ]\n",
    "    \n",
    "    def read_20newsgroups_file(self, path_test_20newsgroups):\n",
    "        data = []\n",
    "        target = []\n",
    "        \n",
    "        with open(path_test_20newsgroups) as file:\n",
    "            for index, line in enumerate(file):\n",
    "                tokens_count = len(line.split())\n",
    "\n",
    "                if tokens_count > 1 and tokens_count <= 301:\n",
    "                    category, text = line.split(None, 1)\n",
    "                    data.append(self.clean_str(text))\n",
    "                    target.append(self.target_names_20newsgroups.index(category))\n",
    "                \n",
    "        return data, target\n",
    "        \n",
    "    \n",
    "    def fetch_20newsgroups(self, subset = \"train\"):\n",
    "        dataset = {'data': None,  'target': None , 'target_names': self.target_names_20newsgroups}\n",
    "    \n",
    "        if subset == 'train':\n",
    "            dataset['data'], dataset['target'] = self.read_20newsgroups_file(self.path_train_20newsgroups)\n",
    "        elif subset == 'test':\n",
    "            dataset['data'], dataset['target'] = self.read_20newsgroups_file(self.path_test_20newsgroups)\n",
    "        elif subset == 'all':\n",
    "            data_train, target_train = self.read_20newsgroups_file(self.path_train_20newsgroups)\n",
    "            data_test, target_test = self.read_20newsgroups_file(self.path_test_20newsgroups)\n",
    "            \n",
    "            dataset['data'], dataset['target'] = data_train + data_test, target_train + target_test\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    #################################################################\n",
    "    ##################### SearchSnippets ############################\n",
    "    #################################################################\n",
    "    \n",
    "    path_train_search_snippets = \"../SearchSnippets/train.txt\"\n",
    "    path_test_search_snippets = \"../SearchSnippets/test.txt\"\n",
    "    \n",
    "    target_names_search_snippets = [\n",
    "        \"business\",\n",
    "        \"computers\",\n",
    "        \"culture-arts-entertainment\",\n",
    "        \"education-science\",\n",
    "        \"engineering\",\n",
    "        \"health\",\n",
    "        \"politics-society\",\n",
    "        \"sports\"\n",
    "    ]\n",
    "    \n",
    "    def read_search_snippets_file(self, path_test_search_snippets):\n",
    "        data = []\n",
    "        target = []\n",
    "        \n",
    "        with open(path_test_search_snippets, encoding=\"utf8\") as file:\n",
    "            for index, line in enumerate(file):\n",
    "                tokens_count = len(line.split())\n",
    "               \n",
    "                if tokens_count > 1 and tokens_count <= 301:\n",
    "                    text, category = line.rsplit(None, 1)\n",
    "                    data.append(self.clean_str(text))\n",
    "                    target.append(self.target_names_search_snippets.index(category))\n",
    "                \n",
    "        return data, target\n",
    "        \n",
    "    \n",
    "    def fetch_search_snippets(self, subset = \"train\"):\n",
    "        dataset = {'data': None,  'target': None , 'target_names': self.target_names_search_snippets}\n",
    "    \n",
    "        if subset == 'train':\n",
    "            dataset['data'], dataset['target'] = self.read_search_snippets_file(self.path_train_search_snippets)\n",
    "        elif subset == 'test':\n",
    "            dataset['data'], dataset['target'] = self.read_search_snippets_file(self.path_test_search_snippets)\n",
    "        elif subset == 'all':\n",
    "            data_train, target_train = self.read_search_snippets_file(self.path_train_search_snippets)\n",
    "            data_test, target_test = self.read_search_snippets_file(self.path_test_search_snippets)\n",
    "            \n",
    "            dataset['data'], dataset['target'] = data_train + data_test, target_train + target_test\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "dict_keys(['data', 'target', 'target_names'])\n",
      "\n",
      "\n",
      "Dataset example:\n",
      "univers violat separ church state dmn kepler unh edu king becom philosoph philosoph becom king write recent ra order and resist care appar post religi flyer entitl soul scroll thought religion spiritu and matter soul insid bathroom stall door school univers hampshir sort newslett assembl hall director campu pose question spiritu each issu and solicit respons includ issu pretti vagu assum put christian care not mention jesu bibl heard defend doesn support religion thi state univers and strong support separ church and state enrag can thi sound scream for parodi give copi your friendli neighbourhood subgeniu preacher luck run mental mincer and hand you back outrag offens and gut bustingli funni parodi you can past origin can stool scroll thought religion spiritu and matter colon you can us thi text wipe mathew\n",
      "\n",
      "\n",
      "Train dataset:\n",
      "Set size: 10443\n",
      "\n",
      "\n",
      "Test dataset:\n",
      "Set size: 6972\n",
      "\n",
      "\n",
      "Mean Lenght: 94.2\n",
      "Max Lenght: 300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "print(\"Dataset structure:\")\n",
    "train_dataset = dataset.fetch_20newsgroups(subset = 'train')\n",
    "print(train_dataset.keys())\n",
    "print(\"\\n\")\n",
    "print(\"Dataset example:\")\n",
    "print(train_dataset['data'][0])\n",
    "print(\"\\n\")\n",
    "print(\"Train dataset:\")\n",
    "print(\"Set size: {}\".format(len(train_dataset['data'])))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "test_dataset = dataset.fetch_20newsgroups(subset = 'test')\n",
    "print(\"Set size: {}\".format(len(test_dataset['data'])))\n",
    "print(\"\\n\")\n",
    "\n",
    "complete_dataset = train_dataset['data'] + test_dataset['data']\n",
    "\n",
    "mean_length = sum(len(document.split()) for document in complete_dataset)/len(complete_dataset)\n",
    "max_length = max(map(lambda document: len(document.split()), complete_dataset))\n",
    "\n",
    "print('Mean Lenght: {}'.format(round(mean_length, 1)))\n",
    "print('Max Lenght: {}'.format(max_length))\n",
    "print(\"\\n\")\n",
    "\n",
    "vocabulary = list()\n",
    "for document in complete_dataset:\n",
    "    for word in document.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "print('Vocabulary Size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SearchSnippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "dict_keys(['data', 'target', 'target_names'])\n",
      "\n",
      "\n",
      "Dataset example:\n",
      "manufacture manufacturer directory directory china taiwan products manufacturers directory taiwan china products manufacturer direcory exporter directory supplier directory suppliers\n",
      "\n",
      "\n",
      "Train dataset:\n",
      "Set size: 10060\n",
      "\n",
      "\n",
      "Test dataset:\n",
      "Set size: 2280\n",
      "\n",
      "\n",
      "Mean Lenght: 18.1\n",
      "Max Lenght: 50\n",
      "\n",
      "\n",
      "Vocabulary Size: 29257\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "print(\"Dataset structure:\")\n",
    "train_dataset = dataset.fetch_search_snippets(subset = 'train')\n",
    "print(train_dataset.keys())\n",
    "print(\"\\n\")\n",
    "print(\"Dataset example:\")\n",
    "print(train_dataset['data'][0])\n",
    "print(\"\\n\")\n",
    "print(\"Train dataset:\")\n",
    "print(\"Set size: {}\".format(len(train_dataset['data'])))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "test_dataset = dataset.fetch_search_snippets(subset = 'test')\n",
    "print(\"Set size: {}\".format(len(test_dataset['data'])))\n",
    "print(\"\\n\")\n",
    "\n",
    "complete_dataset = train_dataset['data'] + test_dataset['data']\n",
    "\n",
    "mean_length = sum(len(document.split()) for document in complete_dataset)/len(complete_dataset)\n",
    "max_length = max(map(lambda document: len(document.split()), complete_dataset))\n",
    "\n",
    "print('Mean Lenght: {}'.format(round(mean_length, 1)))\n",
    "print('Max Lenght: {}'.format(max_length))\n",
    "print(\"\\n\")\n",
    "\n",
    "vocabulary = list()\n",
    "for document in complete_dataset:\n",
    "    for word in document.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "print('Vocabulary Size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
